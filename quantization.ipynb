{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.DEBUG)\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import numpy as np\n",
    "assert float(tf.__version__[:3]) >= 2.3\n",
    "\n",
    "from utils.helpers import get_dataset_info\n",
    "from utils.data_generator import ImageDataGenerator\n",
    "\n",
    "from builders import builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Model\n",
    "#### 참조사이트\n",
    "- [훈련 후 정수 양자화](https://www.tensorflow.org/lite/performance/post_training_integer_quant?hl=ko)\n",
    "- [TensorFlow Lite 8bits 양자화 사양](https://www.tensorflow.org/lite/performance/quantization_spec)\n",
    "- [딥러닝의 Quantization (양자화)와 Quantization Aware Training](https://gaussian37.github.io/dl-concept-quantization/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNet_based_on_MobileNetV2_gamma_2.h5',\n",
       " 'UNet_based_on_MobileNetV2_gamma_5.h5',\n",
       " 'UNet_based_on_MobileNetV2_CE.h5',\n",
       " 'UNet_based_on_VGG16_29.h5',\n",
       " 'UNet_based_on_MobileNetV2_QAT.h5',\n",
       " 'PAN_based_on_MobileNetV2_29.h5',\n",
       " 'DeepLabV3Plus_based_on_MobileNetV2_29.h5',\n",
       " 'UNet_based_on_MobileNetV2_CE_QAT_288384.h5',\n",
       " 'UNet_based_on_MobileNetV2_CE_QAT_288384_aug.h5',\n",
       " 'UNet_based_on_MobileNetV2_CE_QAT_288384_50000.h5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_list = os.listdir('./weights/')\n",
    "weights_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hci\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\layers\\wrappers.py:64: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  config = {'layer': generic_utils.serialize_keras_object(self.layer)}\n",
      "c:\\Users\\hci\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  layer_config = serialize_layer_fn(layer)\n"
     ]
    }
   ],
   "source": [
    "num_classes = 2 # 클래스 갯수\n",
    "size = (288,384) # 이미지 사이즈(높이, 너비)\n",
    "batch_size = 32 # 배치 사이즈\n",
    "model_name = 'UNet' # 모델명\n",
    "base_model = 'MobileNetV2' # base model명\n",
    "layers = tf.keras.layers \n",
    "inputs = layers.Input(shape=size+(3,),dtype=tf.float32) # input : float32\n",
    "\n",
    "working_dir = os.getcwd()\n",
    "\n",
    "quantize_model = tfmot.quantization.keras.quantize_model \n",
    "\n",
    "# 모델 생성\n",
    "model, base_model = builder(num_classes, size, model_name, base_model)\n",
    "\n",
    "# 전체 모델에 QAT 적용\n",
    "# 모델은 양자화를 인식하지만, 양자화되지는 않음(예: 가중치가 int8 대신 float32임)\n",
    "model = quantize_model(model)\n",
    "\n",
    "# 가중치 로드\n",
    "model.load_weights(os.path.join(working_dir,'weights',weights_list[-1]))\n",
    "\n",
    "# 모델의 inputs과 outputs을 정의해줌\n",
    "x = model(inputs)\n",
    "outputs = layers.Softmax(axis=-1)(x) \n",
    "\n",
    "model = tf.keras.Model(inputs=inputs,outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as Conv1_layer_call_fn, Conv1_layer_call_and_return_conditional_losses, Conv1_relu_layer_call_fn, Conv1_relu_layer_call_and_return_conditional_losses, expanded_conv_depthwise_layer_call_fn while saving (showing 5 of 635). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\hci\\AppData\\Local\\Temp\\tmp6ptw2t8p\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\hci\\AppData\\Local\\Temp\\tmp6ptw2t8p\\assets\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    }
   ],
   "source": [
    "# 동적 양자화\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "tflite_model_quant = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  <class 'numpy.float32'>\n",
      "output:  <class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "# 모델의 input과 output의 datatype을 확인\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n",
    "input_type = interpreter.get_input_details()[0]['dtype']\n",
    "print('input: ', input_type)\n",
    "output_type = interpreter.get_output_details()[0]['dtype']\n",
    "print('output: ', output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12931768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 저장\n",
    "import pathlib\n",
    "\n",
    "tflite_models_dir = pathlib.Path(\"./tflite_models/unet_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save the quantized model:\n",
    "tflite_model_quant_file = tflite_models_dir/\"unet_tflite_quant_288384_50000_models.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_model_quant)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow lite Metadata Writer\n",
    "- `TensorFlow Lite Model Metadata`는 표준 모델 설명 형식이다. \n",
    "- 일반적인 모델 정보, 입출력 및 관련 파일에 대한 풍부한 내용을 담고 있다.\n",
    "- tensorflow lite 모델에 inference동안 필요한 정보들을 포함시켜준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflite_support.metadata_writers import image_segmenter\n",
    "from tflite_support.metadata_writers import writer_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['unet_tflite_models.tflite',\n",
       " 'unet_tflite_quant_models.tflite',\n",
       " 'metadata',\n",
       " 'unet_tflite_quant_qat_models.tflite',\n",
       " 'unet_tflite_quant_320480_models.tflite',\n",
       " 'unet_tflite_quant_288384_models.tflite',\n",
       " 'unet_tflite_quant_288384_aug_models.tflite',\n",
       " 'unet_tflite_quant_288384_50000_models.tflite',\n",
       " 'desktop.ini']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tflite_models_dir = './tflite_models/unet_tflite_models'\n",
    "tflite_model_list = os.listdir(tflite_models_dir)\n",
    "tflite_model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"ImageSegmenter\",\n",
      "  \"description\": \"Semantic image segmentation predicts whether each pixel of an image is associated with a certain class.\",\n",
      "  \"subgraph_metadata\": [\n",
      "    {\n",
      "      \"input_tensor_metadata\": [\n",
      "        {\n",
      "          \"name\": \"image\",\n",
      "          \"description\": \"Input image to be segmented.\",\n",
      "          \"content\": {\n",
      "            \"content_properties_type\": \"ImageProperties\",\n",
      "            \"content_properties\": {\n",
      "              \"color_space\": \"RGB\"\n",
      "            }\n",
      "          },\n",
      "          \"process_units\": [\n",
      "            {\n",
      "              \"options_type\": \"NormalizationOptions\",\n",
      "              \"options\": {\n",
      "                \"mean\": [\n",
      "                  127.5\n",
      "                ],\n",
      "                \"std\": [\n",
      "                  127.5\n",
      "                ]\n",
      "              }\n",
      "            }\n",
      "          ],\n",
      "          \"stats\": {\n",
      "            \"max\": [\n",
      "              1.0\n",
      "            ],\n",
      "            \"min\": [\n",
      "              -1.0\n",
      "            ]\n",
      "          }\n",
      "        }\n",
      "      ],\n",
      "      \"output_tensor_metadata\": [\n",
      "        {\n",
      "          \"name\": \"segmentation_masks\",\n",
      "          \"description\": \"Masks over the target objects with high accuracy.\",\n",
      "          \"content\": {\n",
      "            \"content_properties_type\": \"ImageProperties\",\n",
      "            \"content_properties\": {\n",
      "              \"color_space\": \"GRAYSCALE\"\n",
      "            },\n",
      "            \"range\": {\n",
      "              \"min\": 1,\n",
      "              \"max\": 2\n",
      "            }\n",
      "          },\n",
      "          \"stats\": {\n",
      "          },\n",
      "          \"associated_files\": [\n",
      "            {\n",
      "              \"name\": \"label.txt\",\n",
      "              \"description\": \"Labels for categories that the model can recognize.\",\n",
      "              \"type\": \"TENSOR_AXIS_LABELS\"\n",
      "            }\n",
      "          ]\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ImageSegmenterWriter = image_segmenter.MetadataWriter\n",
    "tflite_model_name = tflite_model_list[-2]\n",
    "_MODEL_PATH = os.path.join(tflite_models_dir,tflite_model_name)\n",
    "\n",
    "# Task Library expects label files that are in the same format as the one below.\n",
    "_LABEL_FILE = \"./label.txt\"\n",
    "\n",
    "model_file_name = \"unet_tflite_quant_qat_288384_models_50000_metadata.tflite\"\n",
    "_SAVE_TO_PATH = os.path.join(tflite_models_dir,'metadata',model_file_name)\n",
    "\n",
    "# Normalization parameters is required when reprocessing the image. It is\n",
    "# optional if the image pixel values are in range of [0, 255] and the input\n",
    "# tensor is quantized to uint8. See the introduction for normalization and\n",
    "# quantization parameters below for more details.\n",
    "# https://www.tensorflow.org/lite/models/convert/metadata#normalization_and_quantization_parameters)\n",
    "_INPUT_NORM_MEAN = 127.5\n",
    "_INPUT_NORM_STD = 127.5\n",
    "\n",
    "# Create the metadata writer.\n",
    "writer = ImageSegmenterWriter.create_for_inference(\n",
    "    writer_utils.load_file(_MODEL_PATH), [_INPUT_NORM_MEAN], [_INPUT_NORM_STD],[_LABEL_FILE])\n",
    "\n",
    "# Verify the metadata generated by metadata writer.\n",
    "print(writer.get_metadata_json())\n",
    "\n",
    "# Populate the metadata into the model.\n",
    "writer_utils.save_file(writer.populate(), _SAVE_TO_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "265b42fca52aa215bce445b59267cb7651d4ee126ccc9c18bdebab578812d2a9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
